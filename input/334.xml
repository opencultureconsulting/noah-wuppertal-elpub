<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/  http://www.openarchives.org/OAI/2.0/oai_dc.xsd"  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><id>334</id><datestamp>2018-01-22</datestamp><setSpec>DDC:5:53:530</setSpec>
          <dc:title xml:lang="de">Nichtlineare Methoden zur Quantifizierung von Abhängigkeiten und Kopplungen zwischen stochastischen Prozessen basierend auf Informationstheorie</dc:title>
          <dc:creator>Kaiser, Andreas</dc:creator>
          <dc:date>2003-01-14</dc:date>
          <dc:identifier>http://elpub.bib.uni-wuppertal.de/edocs/dokumente/fb08/diss2002/kaiser</dc:identifier>
          <dc:identifier>http://elpub.bib.uni-wuppertal.de/servlets/DocumentServlet?id=334</dc:identifier>
          <dc:identifier>urn:nbn:de:hbz:468-20030038</dc:identifier>
          <dc:identifier>http://elpub.bib.uni-wuppertal.de/servlets/DerivateServlet/Derivate-428/d080204.pdf</dc:identifier>
          <dc:description xml:lang="en">In order to determine the relation between two stochastic processes&#13;
information theory offers an appropriate framework in which the&#13;
relationships can be interpreted in terms of information. The dependence&#13;
can be measured with the mutual information, giving the amount of&#13;
information which both processes share, i.e. the degree of similarities.&#13;
Mutual information can also give hints for the coupling direction,&#13;
however, due to serial correlation in time the results might be misleading.&#13;
Additionally, only non-coupled systems can be distinguished from&#13;
coupled systems. To determine the coupling directions, the dynamics of the&#13;
processes have to be taken into account which leads to the transfer entropy.&#13;
By considering the past, transfer entropy measures the direct impact&#13;
which the driving process has on the future state of the driven process&#13;
by excluding any influence due to the serial correlations. Based on&#13;
information theory, coupling strength is quantified as the amount of&#13;
effective information transmission from one process to the other.&#13;
Thus, transfer entropy allows to distinguish between unidirectional&#13;
coupling and bidirectional coupling.&#13;
&lt;p&gt;&#13;
While values for mutual information and transfer entropy can be easily&#13;
archived for processes with discrete state space, their estimations&#13;
from finite data sets are difficult. Partitioning the state space, mutual&#13;
information and transfer entropy of the discretised processes converge&#13;
to the corresponding values of the continuous processes if the partitions&#13;
are refined. Furthermore, mutual information shows monotonically increasing&#13;
convergence and thus can be used to reject the assumption of both processes&#13;
being independent. For transfer entropy no similar monotonic convergence&#13;
seems to hold. Kernel estimators represent an alternative approach in order&#13;
to estimate information theoretical quantities. They are easy to implement&#13;
and the bias of the estimators due to serial correlations in the data sets&#13;
can be suppressed easily too.&#13;
&lt;p&gt;&#13;
A special class of stochastic processes are point processes. Here, the&#13;
discrete times at which an event occurs are of interest. Again mutual&#13;
information can be used to quantify dependence between two point processes&#13;
but this time the increments, i.e. the number of events within a certain&#13;
time interval, have to be considered. A weaker measure for dependence is&#13;
the covariance of the increments leading in a special case to the number&#13;
of coincidences. Considering increments, coupling directions can be&#13;
determined with the transfer entropy as well. Unfortunately, due to the&#13;
large bias in the estimators the exact values of the information transmissions&#13;
cannot reliablely be given. When using increments, the time scale on which&#13;
dependence is detected is given by the length of the time intervals. As an&#13;
alternative method inter-event intervals and cross-event intervals are&#13;
introduced which are ordered to one discrete time index congruently. By&#13;
calculating the mutual information between these event intervals dependence&#13;
between point processes is detectable without choosing a certain time scale.</dc:description>
          <dc:subject>500 Naturwissenschaften und Mathematik » 530 Physik » 530 Physik</dc:subject>
          <dc:subject>Fakultät für Mathematik und Naturwissenschaften » Physik » Dissertationen</dc:subject>
          <dc:type>Wissenschaftliche Abschlussarbeiten » Dissertation</dc:type>
          <dc:language>deu</dc:language>
          <dc:format>application/pdf</dc:format>
          <dc:type>Text</dc:type>
        </oai_dc:dc>